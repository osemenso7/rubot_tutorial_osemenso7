{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual SLAM and Navigation Using Gazebo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SLAM (short for Simultaneous Localization and Mapping) techniques, you will be able to execute autonomous navigation with GoPiGo3.\n",
    "\n",
    "SLAM is a technique used in robotics to explore and map an unknown environment while estimating the pose of the robot itself. As it moves all around, it will be acquiring structured information of the surroundings by processing the raw data coming from its sensors.\n",
    "\n",
    "For optimal and easy-to-understand coverage of the topic of SLAM, we will implement a 360º-coverage Laser Distance Sensor (LDS) in the virtual robot. There are low-cost versions of this sensor technology, such as EAI YDLIDAR X4 (available at https://www.aliexpress.com/item/32908156152.html), which is the one we will make use of in the next chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy its files to the ROS workspace to have them available, and leave the rest outside of the src folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp -R ~/Hands-On-ROS-for-Robotics-Programming/Chapter8_Virtual_SLAM ~/rUBotCoop_ws/src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code contains two new ROS packages as follows: \n",
    "- gopigo3_description, which contains the URDF model plus the SDF (Gazebo tags) for a complete, dynamic simulation. This package provides the gopigo3_rviz.launch launch file to interactively visualize the model in RViz.\n",
    "- virtual_slam contains the virtual robot simulation itself, plus the launch files needed to run SLAM in Gazebo.\n",
    "\n",
    "Then, rebuild the workspace so that it is known to your ROS installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/rUBotCoop_ws\n",
    "catkin_make "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROS navigation packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's prepare your machine with the required ROS packages needed for the navigation stack:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install ros-melodic-navigation ros-melodic-amcl ros-melodic-map-server ros-melodic-move-base ros-melodic-urdf ros-melodic-xacro ros-melodic-compressed-image-transport ros-melodic-rqt-image-view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the slam_gmapping package, that the time of writing is already available in its binary version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install ros-melodic-slam-gmapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding sensors to the GoPiGo3 model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, you should have equipped your virtual robot with a differential drive controller that provides the capability to convert velocity commands into rotations of the left and right wheels. We need to complete the model with some sort of perception of the environment. For this, we will add controllers for two common sensors, a two-dimensional camera and an LDS. The first corresponds to the Pi camera of your physical robot, while the second is the unidirectional distance sensor of the GoPiGo3 kit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add the solid of the camera as usual with <visual> tags, but since it is a commercial device, you can a get better look by using a realistic three-dimensional CAD model supplied by the manufacturer or made by someone else in the open source community. The URDF definition is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<link name=\"camera\">\n",
    "  <visual>\n",
    "    <origin xyz=\"0.25 0 0.05\" rpy=\"0 1.570795 0\" />\n",
    "    <geometry>\n",
    "      <mesh filename=\"package://virtual_slam/meshes/piCamera.stl\" scale=\"0.5 0.5 0.5\"/>\n",
    "    </geometry>\n",
    "  </visual>\n",
    "...\n",
    "</link>\n",
    "<joint name=\"joint_camera\" type=\"fixed\">\n",
    "    <parent link=\"base_link\"/>\n",
    "    <child link=\"camera\"/>\n",
    "    <origin xyz=\"0 0 0\" rpy=\"0 0 0\" /> \n",
    "    <axis xyz=\"1 0 0\" />\n",
    "</joint>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the camera technical features using a <gazebo> tag that emulates the behavior of the camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<gazebo reference=\"camera\">\n",
    "  <sensor type=\"camera\" name=\"camera1\">\n",
    "    <update_rate>30.0</update_rate>\n",
    "    <camera name=\"front\">\n",
    "      <horizontal_fov>1.3962634</horizontal_fov>\n",
    "      <image>\n",
    "        <width>800</width>\n",
    "        <height>800</height>\n",
    "        <format>R8G8B8</format>\n",
    "      </image>\n",
    "    <clip>\n",
    "      <near>0.02</near>\n",
    "      <far>300</far>\n",
    "    </clip>\n",
    "    </camera>\n",
    "    <!-- plugin \"camera_controller\" filename=\"libgazebo_ros_camera.so\" -->\n",
    "  </sensor>\n",
    "</gazebo>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "camera images will be published in the /gopigo/camera1/image_raw topic.\n",
    "\n",
    "Launch the ROS visualization tool to check that the model is properly built. Since RViz only represents its visual features—it does not include any physical simulation engine—it is a much lighter environment than Gazebo and you have available all the options to check every aspect of the appearance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch gopigo3_description gopigo3_basic_rviz.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_model_sensors1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first place the robot in Gazebo the same way we did in the previous chapter and enable remote control with the keyboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_basic_world.launch\n",
    "rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key_teleop allows you to remotely control the GoPiGo3 with the arrow keys of your keyboard.\n",
    "\n",
    "Now, launch a node from the image_view package that comes preinstalled with ROS (we are remapping the image topic so that the node takes its data from the camera node topic, /gopigo/camera1/image_raw):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosrun image_view image_view image:=/gopigo/camera1/image_raw "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teleoperate the robot with the arrow keys and you will see the subjective view in the image window:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_Gazebo_Cam1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's obtain the ROS graph with the well-known command, rqt_graph, and have a look at how the topic remapping for the image is handled:\n",
    "\n",
    "Thanks to the mapping argument, image:=/gopigo/camera1/image_raw, the image topic of the image_view package remains implicit and just the /gopigo/camera1/image_raw is visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_Gazebo_rqt_cam.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First to the previous Gazebo process, type:\n",
    "\n",
    "killall gzserver && killall gzclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the solid model of this sensor under the <visual> tag by following the same procedure we covered for the camera. The URDF definition is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<joint name=\"distance_sensor_solid_joint\" type=\"fixed\">\n",
    "    <axis xyz=\"0 1 0\" />\n",
    "    <origin rpy=\"0 0 0\" xyz=\"0 0 0\" />\n",
    "    <parent link=\"base_link\"/>\n",
    "    <child link=\"distance_sensor_solid\"/>\n",
    "</joint>\n",
    "<link name=\"distance_sensor_solid\">\n",
    "    <visual>\n",
    "      <origin xyz=\"0.2 0 0.155\" rpy=\"1.570795 0 1.570795\" />\n",
    "      <geometry>\n",
    "        <mesh filename=\"package://gopigo3_description/meshes/IR_Sensor_Sharp_GP2Y_solid.stl\" scale=\"0.005 0.005 0.005\"/>\n",
    "      </geometry>\n",
    "      <material name=\"red\"/>\n",
    "    </visual>\n",
    "    ...\n",
    "</link>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the sensor technical features using a <gazebo> tag, which you can see refers to the distance_sensor link defined in the preceding snippet (not distance_sensor_solid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<gazebo reference=\"distance_sensor\"> \n",
    "   <sensor type=\"ray\" name=\"laser_distance\">\n",
    "      <visualize>true</visualize>\n",
    "      <update_rate>10</update_rate>\n",
    "      <ray>\n",
    "         ...\n",
    "         <range>\n",
    "            <min>0.01</min>\n",
    "            <max>3</max>\n",
    "            <resolution>0.01</resolution>\n",
    "         </range>\n",
    "      </ray>\n",
    "      <!-- plugin filename=\"libgazebo_ros_range.so\" name=\"gazebo_ros_ir\" -->\n",
    "    </sensor> \n",
    "   </gazebo>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <update_rate> tag specifies that the sensor is read at a frequency of 10 Hz, and the <range> tag sets measured distance values between 10 cm and 3 m at 1 cm resolution.\n",
    "\n",
    "Finally, we add the Gazebo plugin that emulates the behavior of the distance sensor. The following snippet is what substitutes the commented line referring to plugin \"gazebo_ros_ir\" in the preceding code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      <plugin filename=\"libgazebo_ros_range.so\" name=\"gazebo_ros_ir\">\n",
    "         <gaussianNoise>0.005</gaussianNoise>\n",
    "         <alwaysOn>true</alwaysOn>\n",
    "         <updateRate>0.0</updateRate>\n",
    "             <topicName>gopigo/distance_sensor</topicName>\n",
    "             <frameName>distance_sensor</frameName>\n",
    "         <radiation>INFRARED</radiation>\n",
    "         <fov>0.02</fov>\n",
    "       </plugin>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the ROS visualization tool to check that the model is properly built. Since RViz only represents its visual features, it is a much lighter environment than Gazebo and you have available all the options to check every aspect of the appearance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch gopigo3_description gopigo3_basic_rviz.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_model_sensors1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test includes both the distance sensor and the two-dimensional camera. Run the example by using four Terminals, as indicated in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_basic_world.launch\n",
    "rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel\n",
    "rostopic echo /gopigo/distance\n",
    "rosrun image_view image_view image:=/gopigo/camera1/image_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_Gazebo_distance_sensor1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First to the previous Gazebo process, type:\n",
    "\n",
    "killall gzserver && killall gzclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laser Distance Sensor (LDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the solid model of this sensor under the <visual> tag by following the same procedure we covered for the previous sensors. The URDF definition is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<link name=\"base_scan\">\n",
    "    <visual name=\"sensor_body\">\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 0 0\" />\n",
    "      <geometry>\n",
    "        <mesh filename=\"package://gopigo3_description/meshes/TB3_lds-01.stl\" scale=\"0.003 0.003 0.003\"/> \n",
    "      </geometry>\n",
    "      <material name=\"yellow\"/>\n",
    "    </visual>\n",
    "    <visual name=\"support\">\n",
    "      <origin xyz=\"0 0 -0.0625\" rpy=\"0 0 0\" />\n",
    "      <geometry>\n",
    "        <cylinder length=\"0.12\" radius=\"0.1\" />\n",
    "      </geometry>\n",
    "    </visual> \n",
    "</link>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see two <visual> blocks within the <link> element in the preceding snippet: sensor_body is the LDS itself, and support creates the physical interface between the sensor and the robot chassis. The solid model that we are using for the sensor body is the one shown in the following screenshot, which consists of a CAD model in STL format referenced from the <mesh> tag.\n",
    "    \n",
    "Next, we add a <joint> element of <type=\"fixed\"> to attach the sensor assembly to the robot chassis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<joint name=\"scan_joint\" type=\"fixed\">\n",
    "    <parent link=\"base_link\"/>\n",
    "    <child link=\"base_scan\"/>\n",
    "    <origin xyz=\"-0.1 0 0.25\" rpy=\"0 0 0\"/>\n",
    "</joint>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the sensor technical features using a <gazebo> tag that you can see refers to the distance_sensor link defined in the preceding snippet (not distance_sensor_solid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<gazebo reference=\"base_scan\">\n",
    "    <sensor type=\"ray\" name=\"lds_lfcd_sensor\">\n",
    "      <visualize>true</visualize>\n",
    "      <update_rate>5</update_rate>\n",
    "      <ray>\n",
    "        <scan>\n",
    "          <horizontal> <samples>721</samples> ... </horizontal>\n",
    "        </scan>\n",
    "        <range>\n",
    "          <min>0.12</min>\n",
    "          <max>10</max>\n",
    "          <resolution>0.015</resolution>\n",
    "        </range>\n",
    "      </ray>\n",
    "        <!-- plugin name=\"gazebo_ros_lds_lfcd_controller\" filename=\"libgazebo_ros_laser.so\" -->\n",
    "    </sensor>\n",
    "  </gazebo>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <range> tag sets measured distance values between 12 cm and 10 m, as can be found in the technical specification of the EAI YDLIDAR X4. Pay special attention to the <visualize>true</visualize> tag, since, with a sensor like this, with 360º vision, the screen will be filled with rays to show the angle range that it covers. It is recommended to set this to false once you have visually checked that the sensor is working properly.\n",
    "    \n",
    "The <update_rate> tag specifies that the sensor is read at a frequency of 5 Hz, but the specification of the LDS is 5,000 Hz. Why don't we put the actual value? Since the robot will move at low speed, there is no need to have such a high-frequency reading, so we can limit it to only 5 Hz, which will have no impact on the robot behavior. This will require only 55 Kb/s of bandwidth, 1,000 times lower than what the sensor can provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to add the Gazebo plugin that emulates the behavior of the distance sensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<plugin name=\"gazebo_ros_lds_lfcd_controller\" filename=\"libgazebo_ros_laser.so\">\n",
    "        <topicName>/gopigo/scan</topicName>\n",
    "        <frameName>base_scan</frameName>\n",
    "</plugin>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the range values will be published in the /gopigo/scan topic.\n",
    "\n",
    "Finally, launch the ROS visualization tool to check that the model is properly built. Since RViz only represents its visual features, it is a much lighter environment than Gazebo and you have available all the options to check every aspect of the appearance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch gopigo3_description gopigo3_rviz.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_full_model_rviz.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After including the LDS model in the virtual robot, we can proceed to see how it works by running the simulation in Gazebo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_world.launch\n",
    "rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel\n",
    "rostopic echo /scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_gazebo_lidar1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this sensor, it is better to use a Python script that makes the robot wander in the environment while avoiding the obstacles. To do this, we have implemented the following rules in our script: \n",
    "- If there is no obstacle, move forward at a reference speed of 0.8 m/s. \n",
    "- If the range provided by the distance sensor is lower than 2 meters, go back and rotate counter-clockwise until avoiding the obstacle. \n",
    "- Since the distance sensor throws unidirectional measurements, we should check the measurements from the LDS to find if there are obstacles to the sides, and the threshold should be lower than 1.6 meters. If obstacles are detected, go back and rotate counter-clockwise faster to avoid the obstacle and not get stuck on it.\n",
    "\n",
    "This simple algorithm is implemented in the wanderAround.py script, and can be found under the ./virtual_slam/scripts/wanderAround.py folder.\n",
    "\n",
    "First Kill the previous Gazebo process:\n",
    "killall gzserver && killall gzclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_world.launch\n",
    "rosrun virtual_slam wanderAround.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_gazebo_lidar2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kill the previous Gazebo process, type:\n",
    "killall gzserver && killall gzclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLAM allows the robot to build a map of the environment using the following two sources of information: \n",
    "- Robot pose estimation, coming from the internal odometry (rotary encoders) and IMU sensor data \n",
    "- Distance to objects, obstacles and walls, coming from distance sensors, the LDS in particular \n",
    "In its most basic version, a map includes two-dimensional information, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram shows the map generated using SLAM in ROS\n",
    "\n",
    "In such a two-dimensional map, the free areas and occupied areas are drawn in different intensities of gray in 8-bit format (0-255 range).\n",
    "A value of -1 is assigned to unknown areas. \n",
    "\n",
    "Map information is stored using two files: \n",
    "- A .pgm format file, known as portable graymap format. \n",
    "- A .yaml file containing the configuration of the map. \n",
    "See the following example of its content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image: ./test_map.pgm\n",
    "resolution: 0.010000\n",
    "origin: [-20.000000, -20.000000, 0.000000]\n",
    "negate: 0\n",
    "occupied_thresh: 0.65\n",
    "free_thresh: 0.196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most interesting parameters are the last two:  \n",
    "- occupied_thresh = 0.65 means that a cell is considered as occupied if its probability is above 65%. \n",
    "- free_thresh = 0.196 establishes the threshold value below which the cell is considered free, that is, 19.6%. \n",
    "Given the size in pixels of the image, it is straightforward to infer the physical dimension of the cells in the map. This value is indicated by the resolution parameter, that is, 0.01 meter/pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the map using a Gazebo simulation involves employing the following workflow: \n",
    "- Launch the robot model within a modeled environment. \n",
    "- Launch the mapping ROS package. \n",
    "- Launch a special visualization in RViz that lets us see the areas the robot is scanning as it moves. \n",
    "- Teleoperate the robot to make it cover as much as possible of the surface of the virtual environment. \n",
    "- Once the exploration is finished, save the map, generating the two files in the formats indicated in the preceding section, that is, .pgm and .yaml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your robot has generated a map, it will use it to plan a path to a given target destination. The process of executing such a plan is called navigation, and involves the following steps: \n",
    "- Launch the robot model within the modeled environment. This step is the same as the first step in the SLAM process described earlier. \n",
    "- Provide the costmap that the robot built before. Bear in mind that the map is a characteristic of the environment, not of the robot. Hence, you can build the map with one robot and use the same map in navigation for any other robot you put in the same environment.\n",
    "- Set up the navigation algorithm. We will use the Adaptive Monte Carlo Localization (AMCL) algorithm, the most common choice for effective navigation.\n",
    "- Launch a RViz visualization that will let you visualize the robot in the environment and easily mark the target pose (position and orientation) that it should achieve. \n",
    "- Let the robot navigate autonomously to the target location. At this point, you can relax and enjoy watching how the GoPiGo3 drives to the indicated position while avoiding the obstacles and minimizing the distance it has to cover. \n",
    "Should you want the robot to navigate to another location, you just have to indicate it in RViz once it has reached the previous target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practising SLAM and navigation with the GoPiGo3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's follow these steps to build the map of a simple Gazebo world called stage_2.world: \n",
    "1. Launch the robot model within a modeled environment by running the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_world.launch world:=stage_2.world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Launch the SLAM mapping ROS package, including an RViz visualization that superimposes the virtual model of the robot with the actual scan data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_slam.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Teleoperate the robot to make it cover as much as possible of the surface of the current Gazebo world. Let's do this as usual with the teleoperation package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you move the robot, the LDS sensor will acquire scan data from the unknown areas, and you will receive feedback in the RViz window.\n",
    "\n",
    "4. Once you've finished the exploration, save the map, generating two files of the formats indicated in the preceding SLAM process subsection, that is, .pgm and .yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosrun map_server map_saver -f ~/rUBotCoop_ws/map_stage_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get two files in the root folder of your workspace: map_stage_2.pgm and map_stage_2.yaml.\n",
    "\n",
    "Provided with the map, we are ready to perform robot navigation with the GoPiGo3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_slam_map1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driving along a planned trajectory using navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, close all open Terminals. Then, as in the SLAM process, let's proceed step by step to perform some navigation: \n",
    "Kill the previous Gazebo process:\n",
    "killall gzserver && killall gzclient\n",
    "\n",
    "1. Launch the robot model within the modeled environment. This step is the same as the first step in the SLAM process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_world.launch world:=stage_2.world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set up the navigation algorithm and launch RViz. We will use AMCL, the most common choice for effective navigation.\n",
    "\n",
    "In this step, we also provide the costmap that the robot built before. To do this, you just have to reference the .yaml map file you created before. Make sure that the corresponding .pgm file has the same name and is placed in the same location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_navigation.launch map_file:=$HOME/rUBotCoop_ws/map_stage_2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The RViz window, shown in the following screenshot, lets you visualize the robot in the environment and mark the target pose (position and orientation) that it should achieve:\n",
    "First of all, you have to tell the robot that this is the initial pose by pressing the 2D Pose Estimate button. Then, mark it on screen (in this particular case, it isn't necessary, since the initial pose is the same as the one the robot had when it started to build the map)\n",
    "\n",
    "Afterward, you can press 2D Nav Goal button and set the target to the bottom-left corner by clicking the left mouse button. Release the mouse when the arrow has the desired orientation. After releasing, the robot will compute the path to follow and start navigating autonomously.\n",
    "\n",
    "The orientation of the red arrow tells the GoPiGo3 in what direction it should stay facing once it has arrived at the target, and the curved line going from the robot to the target is the planned path. Since it has a map of the environment available, the robot is able to plan a path that avoids the obstacles.\n",
    "\n",
    "The blue square around the robot represents the local window for obstacle avoidance planning. This is used by the Dynamic Window Approach (DWA) method, which generates a local path that efficiently evades the obstacles. The DWA method performs the calculations taking into account the robot's dynamics, in particular, its limited velocity and acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_slam_map2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
